# dags/export_to_minio.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from minio import Minio
import pandas as pd
import io

def upload_to_minio(**context):
    # 1. Simula dados processados pela DAG
    data = [
        {"user_id": 1, "valor": 100.5, "data_execucao": datetime.now().isoformat()},
        {"user_id": 2, "valor": 200.0, "data_execucao": datetime.now().isoformat()},
    ]
    df = pd.DataFrame(data)

    # 2. Configura cliente MinIO
    client = Minio(
        "minio:9000",
        access_key="minioadmin",
        secret_key="minioadmin",
        secure=False
    )

    # 3. Cria bucket bronze, se não existir
    if not client.bucket_exists("bronze"):
        client.make_bucket("bronze")

    # 4. Gera o nome do arquivo
    file_date = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    parquet_path = f"bronze/dag_processados/{file_date}.parquet"
    json_path = f"bronze/dag_processados/{file_date}.json"

    # 5. Converte em Parquet e faz upload
    parquet_buffer = io.BytesIO()
    df.to_parquet(parquet_buffer, index=False)
    parquet_buffer.seek(0)
    client.put_object("bronze", f"dag_processados/{file_date}.parquet", parquet_buffer, length=-1, part_size=10*1024*1024)

    # (Opcional) também salva JSON
    json_buffer = io.BytesIO(df.to_json(orient="records").encode("utf-8"))
    client.put_object("bronze", f"dag_processados/{file_date}.json", json_buffer, length=len(json_buffer.getvalue()))

    print(f"✅ Dados enviados para MinIO em {parquet_path} e {json_path}")

with DAG(
    dag_id="export_to_minio",
    start_date=datetime(2025, 1, 1),
    schedule_interval="@daily",
    catchup=False,
    tags=["bronze", "minio"],
) as dag:

    export = PythonOperator(
        task_id="upload_to_minio",
        python_callable=upload_to_minio,
        provide_context=True
    )
