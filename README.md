
# SPTrans Pipeline

Este projeto implementa um pipeline de dados que coleta informaÃ§Ãµes da API Olho Vivo da SPTrans, processa os dados com Python e os armazena em um banco de dados PostgreSQL. A orquestraÃ§Ã£o Ã© feita com Apache Airflow, e o ambiente Ã© totalmente containerizado com Docker Compose.

## ğŸš€ VisÃ£o Geral da Arquitetura

## ğŸ§° Tecnologias Utilizadas

- Python 3.11
- Pandas
- Requests
- SQLAlchemy
- Psycopg2
- PostgreSQL 15
- PgAdmin 4
- Apache Airflow 2.7.1
- Docker & Docker Compose

## ğŸ“ Estrutura do Projeto

```
sptrans_pipeline/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ autenticacao.py
â”‚   â””â”€â”€ consulta_linhas_zona_sul.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ main_dag_runner.py
â”‚       â””â”€â”€ sptrans_dag.py
â””â”€â”€ docker-compose.yml
```

## âš™ï¸ Como Executar

1. Clone o repositÃ³rio:
```bash
git clone git@github.com:wellingtonpawlino/sptrans_pipeline.git
cd sptrans_pipeline
```

2. Configure o arquivo `.env` com as variÃ¡veis de conexÃ£o:
```env
DB_HOST=db
DB_PORT=5432
DB_NAME=sptrans
DB_USER=postgres
DB_PASSWORD=postgres
```

3. Construa e inicie os containers:
```bash
docker-compose up --build -d
```

4. Acesse os serviÃ§os:
- Airflow: [http://localhost:8080](http://localhost:8080)
- PgAdmin: [http://localhost:5050](http://localhost:5050)

## ğŸ“… Agendamento com Airflow

O DAG `sptrans_pipeline_dag` Ã© executado diariamente Ã s 4h da manhÃ£ e chama o script principal que coleta e salva os dados da Zona Sul de SÃ£o Paulo.

## ğŸ—ƒï¸ Banco de Dados

Os dados sÃ£o armazenados na tabela `linhas_zona_sul` com os seguintes campos:
- `cl`: cÃ³digo da linha
- `lc`: circular
- `lt`: nÃºmero da linha
- `sl`: sentido
- `tp`: tipo

## ğŸ“Œ ObservaÃ§Ãµes

- O projeto utiliza `.dockerignore` para evitar conflitos e otimizar o build.
- O Airflow Ã© executado em containers separados e nÃ£o precisa estar no `requirements.txt` da aplicaÃ§Ã£o.

## ğŸ‘¨â€ğŸ’» Autores

Wellington Santos

